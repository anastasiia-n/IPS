{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is so sad for my APL friend.............\n",
      "0 I missed the New Moon trailer...\n",
      "1 omg its already 7:30 :O\n",
      "0 .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\n",
      "0 i think mi bf is cheating on me!!!       T_T\n",
      "0 or i just worry too much?\n",
      "1 Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
      "0 Sunny Again        Work Tomorrow  :-|       TV Tonight\n",
      "1 handed in my uniform today . i miss you already\n",
      "1 hmmmm.... i wonder how she my number @-)\n",
      "Total items: 1578614\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "with open('./sentiment-analysis-dataset.csv', 'rt', encoding='utf8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    # skip the header\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        # remove whitespaces at the beginning/end\n",
    "        train_x.append(row[3].strip())\n",
    "        train_y.append(row[1])\n",
    "# let's see the data\n",
    "for i in range(10):\n",
    "    print(train_y[i], train_x[i])\n",
    "    \n",
    "print('Total items:', len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anastasiia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "text = \". \".join(train_x)\n",
    "# default filter: '!\"#$%&()*+,-./:;<=>?@[]^_`{|}~'\n",
    "# (list of characters to filter out)\n",
    "# includes basic punctuation, tabs, and newlines.\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691215\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "del words\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides Tokenizer class, check it [here](https://keras.io/preprocessing/text/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "# since our vocabulary size is very big, let's use 5000 most common words\n",
    "tokenizer.fit_on_texts(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can use 4 attributes:\n",
    "- word_counts(dict): the words and their counts.\n",
    "- word_docs(dict): the words and how many documents(here: list cells) each appeared in\n",
    "- word_index(dict): the words and their unique indexes\n",
    "- document_count(int): a count of the number of documents(here: our list size) that were used to fit the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578614\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.document_count)\n",
    "# the out is the list size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to replace the words with numbers for training.  \n",
    "Let's check the content of the word_index and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 elements {'i': 1, 'to': 2, 'the': 3, 'a': 4, 'my': 5}\n",
      "last 5 elements {'7qir4': 691211, 'imstorm': 691212, 'ri9qn': 691213, '1a8jxz': 691214, \"laws's\": 691215}\n"
     ]
    }
   ],
   "source": [
    "print('first 5 elements', {k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[:5]})\n",
    "print('last 5 elements', {k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[-5:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_index.npy', tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the dictionary we have **all** the unique words from our dataset. When we will use any thansformative method - Tokenizer will use *only 5000* most popular words.  \n",
    "Let's replace our words with one-hot matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-91b5e2ee3992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_x_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \"\"\"\n\u001b[0;32m      8\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anastasiia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36msequences_to_matrix\u001b[1;34m(self, sequences, mode)\u001b[0m\n\u001b[0;32m    332\u001b[0m                              'before using tfidf mode.')\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x = tokenizer.texts_to_matrix(train_x, mode='binary')\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
